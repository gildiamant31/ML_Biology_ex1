---
title: "Tirgul 5 - Decision Trees"
author: "Romi Goldner Kabeli"
date: "Apr 26th, 2021"
output:
  pdf_document: default
  html_document: default
---

A bank has asked us to create an algorithm to help it decide whether they should give a loan to its customers. Since this algorithm must be presented at a board meeting, we are required to choose one that can be presented visually to investors from a non-technical background. Decision trees are perfect for a case like this.

The loan data was obtained from Germany so the currency is recorded in Deutsche Marks (DM).

Set up out environment and data:

```{r setup, results='hide'}
library(gmodels)
library(C50)

# read data
credit <- read.csv("credit.csv")

# Consider setting a seed for reproducible results
set.seed(1234)
```

Quick look at the data (can you guess which will be most important?) :

```{r}
str(credit)
```

Convert the class ("default") to a factor, as it is required by the C50 package.

```{r}
# convert credit from character variable to factor
credit$default<-as.factor(credit$default)
```


```{r}
# look at table of savings/checking balance
table(credit$savings_balance)
table(credit$checking_balance)
```

The checking and savings account balance may prove to be important predictors of
loan default status.

```{r}
# look at summary of loan amount and duration
summary(credit$amount)
summary(credit$months_loan_duration)
```

The loan amounts ranged from 250 DM to 18,424 DM across terms of 4 to 72 months
with a median duration of 18 months and an amount of 2,320 DM.

The 'default' vector indicates whether the loan applicant was unable to meet the
agreed payment terms and went into default. A total of 30 percent of the loans in
this dataset went into default:

```{r}
table(credit$default)
```

Lets split into training and test sets:

```{r}
# sample 800 observations out of the total 1000
train_sample <- sample(1000, 800)

str(train_sample) # the resulting train_sample object is a vector of 800 random integers

# split into train/test
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]

# check that we got about 30% defaulted loans in each dataset:
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
```

The 17th column of the dataset is the default class variable, so we need to exclude it from the training data frame, but supply it as the target factor (label) vector for classification:

```{r}
# apply model in training data (17th column is the label to be predicted)
credit_model <- C5.0(credit_train[-17], credit_train$default)

credit_model
```

The preceding text shows some simple facts about the tree, including the function
call that generated it, the number of features (labeled predictors), and examples
(labeled samples) used to grow the tree. Also listed is the tree size of 59, which
indicates that the tree is 59 decisions deep - quite a bit larger than the example
trees we've considered so far!


Next we'll look at the summary of the model. 
Note that the first three lines could be represented in plain language as:

1. If the checking account balance is unknown or greater than 200 DM, and other_credit
is none/store then classify as "not likely to default."
2. Otherwise, the checking account balance is less than zero DM or between
one and 200 DM, and the credit history is perfect or very good, and housing
is rent or other, then classify as "likely to default."

```{r}
summary(credit_model)
```

The numbers in parentheses indicate the number of examples meeting the criteria for
that decision, and the number incorrectly classified by the decision. 
For instance, on the first line, 326/34 indicates that of the 326 examples reaching the decision, 34 were incorrectly classified as not likely to default. In other words, 34 applicants actually defaulted, in spite of the model's prediction.

As we now know, it is very important to evaluate our model performance:

```{r}
# apply model on test data
credit_pred <- predict(credit_model, credit_test)

CrossTable(credit_test$default, credit_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
```

The performance here is somewhat worse than its performance on the
training data, but not unexpected, given that a model's performance is often worse
on unseen data. Also note that there are relatively many mistakes where the model predicted not a default, when in practice the loaner did default. 
Unfortunately, this type of error is a potentially costly mistake, as the bank loses money on each default. 
Let's see if we can improve the result with a bit more effort.

### Adaptive Boosting

So, our model is not that great. Let's try and improve it next. C5.0 includes a feature called adaptive boosting. This is a process in which many decision trees are built and the trees vote on the best class for each example. 

The C5.0() function makes it easy to add boosting to our C5.0 decision tree. We
simply need to add an additional trials parameter indicating the number of
separate decision trees to use in the boosted team. The trials parameter sets an
upper limit; the algorithm will stop adding trees if it recognizes that additional 
trials do not seem to be improving the accuracy. 
We'll start with 10 trials, a number that has become the de facto standard, as research 
suggests that this reduces error rates on test data by about 25 percent:

```{r}
# boosting with 10 trials (on training)
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)

credit_boost10

summary(credit_boost10)
```

The classifier made 25 mistakes on 800 training examples for an error rate of
3.1% percent. This is quite an improvement over the previous training error rate
before adding boosting! However, it remains to be seen whether we see
a similar improvement on the test data. Let's take a look:

```{r}
# boosting on test data
credit_boost_pred10 <- predict(credit_boost10, credit_test)

CrossTable(credit_test$default, credit_boost_pred10,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
```

The model is still not doing well at predicting defaults, which may be a result of our relatively small training dataset, or it may just be a very difficult problem to solve.

Next, lets proceed to fine-tune our algorithm, using a cost matrix. 
The C5.0 algorithm allows us to assign a penalty to different types of errors, in order to discourage a tree from making more costly mistakes. The penalties are designated in a cost matrix, which specifies how much costlier each error is, relative to any other prediction.

First, we'll create a default 2x2 matrix, to later be filled with our cost values:

```{r}
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")

matrix_dimensions
```

Suppose we believe that a loan default costs the bank four times as much as a missed opportunity. 
Our penalty values could then be defined as:

```{r}
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)

error_cost
```

Now lets train again and see if the cost matrix made any difference:

```{r}
# apply model on training data with cost matrix
credit_cost <- C5.0(credit_train[-17], credit_train$default, costs = error_cost)

# predict on test data
credit_cost_pred <- predict(credit_cost, credit_test)

CrossTable(credit_test$default, credit_cost_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
```

Compare these results to the boosted model; this version makes more mistakes overall, but the types of mistakes are very different. Where the previous models incorrectly classified a small number of defaults correctly, our weighted model has does much better in this regard. This trade resulting in a reduction of false negatives at the expense of increasing false positives may be acceptable if our cost estimates were accurate.


To create our decision trees in this practice we used the C5.0 package. 
There is another package called "party" which has the 'ctree' function which
also generates decision trees. You can read about it here: https://ademos.people.uic.edu/Chapter24.html.

