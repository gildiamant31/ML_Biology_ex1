---
title: "ML_B final project - Cancer patients Dataset"
author: "Gil Diamant, Yishay Schlesinger & Itamar Twersky"
date: "17 05 2022"
output: pdf_document
---

```{r setup, global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.width=8, fig.height=3)
```

load libraries 

```{r echo=FALSE}
set.seed(10)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots
library(ggplot2)      # load the ggplot2 library
library(readxl)
library(scales)
library(gmodels)
library(C50)
library(mlbench)
library(caret)
library(corrplot)
library(vip)          # for variable importance plots
library(klaR)
library(psych)
library(cowplot)
library(MASS)
library(devtools)
#install_github('fawda123/ggord')
library(ggord)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(purrr)
library(class)
```
```{r echo=FALSE}
##### Theme Moma #####
theme_moma <- function(base_size = 12, base_family = "Helvetica") {
  theme(
    plot.background = element_rect(fill = "#F7F6ED"),
    legend.key = element_rect(fill = "#F7F6ED"),
    legend.background = element_rect(fill = "#F7F6ED"),
    panel.background = element_rect(fill = "#F7F6ED"),
    panel.border = element_rect(colour = "black", fill = NA, linetype = "dashed"),
    panel.grid.minor = element_line(colour = "#7F7F7F", linetype = "dotted"),
    panel.grid.major = element_line(colour = "#7F7F7F", linetype = "dotted")
  )
}
```
### Have a look at the data
Read the data to a local variable
Firstly,Read the data and lets observe the properties of the data:

```{r}
# read the data
cancer_patient <- read_excel("cancer patient data sets.xlsx")
#show data as str
str(cancer_patient)
```
The data is about classified the level of the cancer disease for a patient, determined by 23 various aspects of the patient (from age and gender to snoring or weight loss level).
As we can see there is 3 cancer levels "High"/"Medium"/"Low" which are defined as a strings labels. For each patient there are various aspects
which are defined as integers 1-10 (exclude age(numeric),and gender(1-male/2-female)). Each patient is defined by id string.
From the table size we can see that we have 1000 patients.

Now, we want to check data clarity, and to check if there is NA/inf value.(if so, to "clean" the data).
Moreover, we want to check the classes balancing.

```{r}
any(is.na(cancer_patient))
total = c()
for (variable in cancer_patient) {
  total <- append(total, any(is.infinite(variable)))
}
any(total)
prop.table(table(cancer_patient$Level))
```
From the results, we can see that -
1. We don't have any NA/inf values, so the data clarity is fine.
2. The classes are balanced, so the data is almost evenly distributed, and we should preserve it when splitting the data.

## EDA
After looking on our data set, we had some exploratory questions and we want to see whats the data can tell about them.
Firstly, we asked if cancer condition is different between genders in our data

```{r}
 # convert Type from numeric variable to factor (factor is an ordered object)
 cancer_patient$Level<-as.factor(cancer_patient$Level)
 #change gender from number to word
 cancer_patient_gender = cancer_patient
 cancer_patient_gender$Gender[cancer_patient_gender$Gender==1] = "Male"
 cancer_patient_gender$Gender[cancer_patient_gender$Gender==2] = "Female"
 # create two plots
 pM = ggplot(cancer_patient_gender[cancer_patient_gender$Gender=="Male",],
             aes(x = factor(1) , fill = Level)) +
   geom_bar(width = 1) + facet_grid(cols = vars(Gender)) + 
   coord_polar(theta = "y")+ xlab("") + ylab("") 
 pF = ggplot(cancer_patient_gender[cancer_patient_gender$Gender=="Female",],
             aes(x = factor(1) , fill = Level)) +
   geom_bar(width = 1) + facet_grid(cols = vars(Gender)) + 
   coord_polar(theta = "y")+ xlab("") + ylab("")  + ggtitle("")
 # show plots together
 title <- ggdraw() + 
  draw_label("cancer level vs gender",fontface = 'bold',x = 0,hjust = 0)
 plot_row <- plot_grid(pM, pF)
 plot_grid(title, plot_row,ncol = 1,rel_heights = c(0.5, 1))
```
As shown it the graph above, the males have much bigger fraction of severe cancer.
The difference can cause from many reasons, we wish to find a feature that connected both to men and cancer, and can partly explain the difference between genders.

Smoking was our chosen feature because we know that smoking have strong connection to cancer and we thought that maybe men tend to smoke more.
So the next test is if men tend to smoke more:
```{r}
ggplot(data = cancer_patient_gender, aes(x=Gender, y=Smoking, fill=Gender)) + geom_boxplot()+
  theme_bw() +
  labs(title = "smoking by gender")
```
As shown above, we saw that actually men tend to smoke more and the can partially explain the difference in the illness.

last thing for the EDA, we ask if  snoring people is tend to be more tired because of difficulty in breathing in their Sleep.
```{r}
# check correlation between Snoring and Fatigue
library("ggpubr")
ggscatter(cancer_patient, x = "Snoring", y = "Fatigue", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Snoring", ylab = "Fatigue") + ggtitle("corralation of Snoring & Fatigue")
```
Surprisingly we saw no significant correlation.
## dimension reduction - PCA
Our data contains many columns which means its have high dimension. 
We wish to transfer our data to 2D while Preserving our high dimension variation.
Having our data in 2D will allow us to see if the 3 cancer levels in our data make 3 different distinct groups.
In addition, we will be able to show the the next algorithms results on our data using the 2D presention of our data.
* Important to mention that it is important to preprocess your data- in this case our data has only numeric values, as we shown before. 

We choose PCA algorithm which is reliable dimensional reducerfor showing our data in 2D graph.
```{r}
# Preform PCA on our data
prcomp_cancer_patient <- prcomp(cancer_patient[, 2:24])
# bind between 2 first pcas and our dataframe
pca_cancer_patient <-  cbind(cancer_patient,
                             PC1 = prcomp_cancer_patient$x[, 1],
                             PC2 = prcomp_cancer_patient$x[, 2])
```
lets plot our pca results:
```{r}
# plot the types according two specific columns after PCA
ggplot(pca_cancer_patient, aes(y=PC1, x = PC2, col=Level)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("cancer patient Dataset")

```
pca gave as moderate results,with each group are dominant in certain area, but the groups are not completely separated. Lets check our variance - explaining in the 4 firsts pc components
```{r}
# variance of the 4 firsts pc components
print(round(prcomp_cancer_patient$sdev[1:4] / sum(prcomp_cancer_patient$sdev) * 100, 2))
```
as showed above - the pc components explain small part of the variance in the data, and this is probabaly the cause to the moderate success in the separation of the samples.
Maybe our data are not lineary separable, and pca as a linear algorithem having a hard time seprate it, we will check this later.
## classification
Now, We are interested in creating a model with the ability to predict the level of the cancer for a patient, by its properties.
Having good model can allow us to predict new patients' condition according to their symptoms.  
#### prepreing the data
Lets shuffle the data so it's order Will not affect our models 
```{r}
# shuffle the data by rows
set.seed(1234)
pca_cancer_patient= pca_cancer_patient[sample(1:nrow(pca_cancer_patient)), ]
```
We saw above that the range of value is different between properties. ??? (all of them are 1-10)
Because we don't want that it will affect the importance of each property, we standardize the data with z-score.(to standard distribution)
```{r}
# z-score standardization (on the feuters columns only)
cancer_patient_z <- cbind(as.data.frame(scale(pca_cancer_patient[2:24])),pca_cancer_patient[25:27])
```
## KNN:
The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve classification problems.
The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.
KNN is looking for the 'k' nearest neighbors to the Unlabeled sample and classify it Like most neighbors, while 'k' is  a hyper-parameter we should tune.

lets try it on out data:
The next step is to create a training set and a test set, and to store the labels in a different vector to be used later. We divide our data into 80% train, 20% test.

```{r}
# create train set
cancer_train_z <- cancer_patient_z[1:800, ]
# create test set
cancer_test_z <- cancer_patient_z[801:1000, ]
# labels of train set
cancer_train_labels <- cancer_patient_z$Level[1:800]
# labels of test set
cancer_test_labels <- cancer_patient_z$Level[801:1000]
```
Lets check that the classes distribution is preserved between train and test:
```{r}
prop.table(table(cancer_train_labels))
prop.table(table(cancer_test_labels))
```
As we can see, the distribution is preserved.

And finally, we are ready to run our algorithm. We'll start with K=3 as arbitrary hyper-parameter, and check another options if its wont be good.
```{r}
# get predictions for the test set from knn,
#send as input the features columns and the train labels 
cancer_test_pred_z <- knn(train = cancer_train_z[1:23],
                          test = cancer_test_z[1:23], 
                          cl = cancer_train_labels, k=3)
# check results
correction <- (cancer_test_pred_z == cancer_test_z$Level)
"TRUE"
table(correction)[["TRUE"]]
"FALSE"
200 - table(correction)[["TRUE"]]
```
All of the patients were predicted correctly- k=3 is the best we can do. 

Now, we want to check another classification algorithm. but this time we want to check other approach. The KNN working by clustering, and now we want to work by a "learnable" vector separation model, so we will use SVM.

##  SVM:
A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes.
SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes.
SVMs are more commonly used in classification problems and as such, we will use it here on our level of cancer classification.

we will start with c=0.1 and gamma = 10 as arbitrary hyper-parameters -

```{r}
# fit model
classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "radial", cost = 0.1, gamma = 10)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])
# check  true,false classifying rate
correction <- (svm_pred == cancer_test_z$Level)
"TRUE"
table(correction)[["TRUE"]]
"FALSE"
200 - table(correction)[["TRUE"]]
```
There is some mistkaes, lets check it more deeply:
```{r}
# show preformence in table
CrossTable(x = cancer_test_labels, y = svm_pred, prop.chisq=FALSE,format ="SPSS")
# check  true,false classifying rate
correction <- (svm_pred == cancer_test_z$Level)
"TRUE"
table(correction)[["TRUE"]]
"FALSE"
200 - table(correction)[["TRUE"]]
#add to test the preicted values
test_v_1 <- cbind(cancer_test_z,svm_pred)
#This will return TRUE or FALSE.
test_v_1$Correct <- test_v_1$Level == test_v_1$svm_pred
# again here we also plot the types according two PCA with TRUE or FALSE labeling, and color of the true lable
ggplot(test_v_1, aes(y=PC1, x = PC2, col=cancer_test_labels,
                     label = Correct, shape = Level)) +
  geom_point(size=5, alpha = 0.2) + 
  scale_color_manual(values=c("#000000","#FF0000","#00BA00")) +
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("SVM Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
```
We got 3 mistakes on the test, so now we want to find better hyper-parameters.
#### tune cost and gama hyper prameters
Trying range of gamma and cost:
```{r}
# tune model to find optimal cost and gamma values
tune.out <- tune(svm, Level~ ., data = cancer_train_z[1:24],
                 kernel = "radial",ranges = list(gamma = c(0.1, 1, 5, 10, 100),
                 cost = c(0.1, 1, 5, 10, 100)),
                 tunecontrol = tune.control(sampling = "fix"))
# show best model parameters
print(tune.out$best.parameters)
```
From the results we can see the c=0.1 and gamma=0.1 are better hyper-parameter, so lets run with them -
(hoping we will get 100% accurate like the knn model)
```{r}
# fit model
classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "radial", cost = 0.1, gamma = 0.1)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])

# check if its 100%  show true,false classifing rate
correction <- (svm_pred == cancer_test_z$Level)
"TRUE"
table(correction)[["TRUE"]]
"FALSE"
200 - table(correction)[["TRUE"]]
```
Now we have 0 mistakes, so we got excellent results with SVM.

We saw very great results in KNN and radial SVM, but moderate separation in the PCA(although PCA is not classifier, we expect to see good separation of the samples by class which is not so good). We suggested that it may cause by the data which is cannot be separated linearly, so we want to test linear SVM to try to check our hypothesis.
#### linear svm
```{r}
# fit model
classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "linear", cost = 0.1, gamma = 0.1)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])
# check  true,false classifying rate
correction <- (svm_pred == cancer_test_z$Level)
"TRUE"
table(correction)[["TRUE"]]
"FALSE"
200 - table(correction)[["TRUE"]]
```
From the results we can see that we have 0 mistakes, which not support our hypothesis.

Now, After KNN and SVM, we wanted to test a model that gives us information about patient's properties importance for the level of cancer, which we can learn from the output of the model.
We choose Decision-Tree model for our next model -
##  Decision-Tree classifer:
Decision Tree algorithm belongs to the family of supervised learning algorithms. The decision tree algorithm can be used for solving regression and classification problems too.
The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).
In Decision Trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with the record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.

we already have training and test sets, normalized and shuffled,
lets run our DT:
```{r}
set.seed(1234)
# apply model in training data (8th column is the label to be predicted)
cancer_model <- C5.0(cancer_train_z[1:23], cancer_train_labels)
#take a look at our model
cancer_model %>% vip()
```
We can see that our tree relay hard on only 5 of the features and maybe its decrease our performance - lets check the accuracy.

Next we'll look at the summary of the model.
```{r}
summary(cancer_model)
```
We got 100% accuracy on train data.

As we now know, it is very important to evaluate our model performance on the test set:
```{r}
set.seed(1234)
# apply model on test data
cancer_pred <- predict(cancer_model, cancer_test_z[1:23])
# check  true,false classifying rate
correction <- (cancer_pred == cancer_test_z$Level)
"TRUE"
table(correction)[["TRUE"]]
"FALSE"
200 - table(correction)[["TRUE"]]
```
We got 100% accuracy on test data.

So Another way to look about our tree relay hard on only 5 of the features, is that we can see that we got 100% accuracy relaying on those features so we can said that the patient's level of the cancer can be predicted by its Passive Smoker, Fatigue, Wheezing, Obesity and snoring features level, and it is not decrease our performance as we thought before.

We did dimensionality reduction using PCA and classification using 3 algorithms. 
Finely, we want to try another dimensionality reduction to see if its can sepreate the data better, in adittion we would like to check if
LDA can classify the data correctly according to its separation.

## LDA
Linear discriminant analysis (LDA), is used as a tool for classification, dimension reduction, and data visualization. It is try to maximize the distance between the means and to minimize the variation.

Lets run LDA, and check how the ldas explaining our data:
```{r}
# apply LDA
linear <- lda(Level~., cancer_train_z[1:24])
#  calculate lda poprportins
lda_porpotions = signif(prop.table(linear$svd^2),3)
# take a look to see how the ldas explaining our data:
print("LDA1   LDA2")
print( paste0(as.character(lda_porpotions[1]),"  ", as.character(lda_porpotions[2])))
```
The LDA1 is high (0.888) which show that it has the best variation between categories that LDA create, and LDA2 is seconed one. Both of them are the vast majority to explain the variation between the categories for us (in contrast to the PCA1 and PCA2 we saw before). 

### bi-plot
Lets view the LDA1/2 plot -
```{r}
ggord::ggord(linear, cancer_train_z$Level, ylim = c(-5, 5),txt = NULL)
```
From the plot we can see very good separation(in which LD1 have the most significant effect - the seperation is mostly horizontaly), although its not perfect, which corresponding to the results above.

### Confusion matrix and accuracy – training data
Lets get the classification on the data, start with the 'train' - 
```{r}
# prediction - training data
p1 <- predict(linear, cancer_train_z[1:23])$class
tab <- table(Predicted = p1, Actual = cancer_train_z$Level)
tab
sum(diag(tab))/sum(tab)
```
From the results we can see that we have 32 mistakes, 31 of them are Medium that classified as Low, which corresponding to the plot above (see the green(Low) and the blue(Medium) circles). As so, the accuracy 96%, good but not perfect.

### Confusion matrix and accuracy – testing data
Lets get the classification on the 'test' data - 

```{r}
# prediction - test data
p2 <- predict(linear,  cancer_test_z[1:23])$class
tab1 <- table(Predicted = p2, Actual = cancer_test_z$Level)
tab1

sum(diag(tab1))/sum(tab1)
```
We can see that on the test we have 10 mistakes, again Medium that classified as Low, which corresponding to the plot above and to the train classification by the LDA. The accuracy is 95%,good but not perfect, corresponding to the train.

Those results can imply us that the linear algorithm is not the best for our data.

## Summary & Conclusions :

firstly, we analyze the data, and understood it. The data is about classified the cancer disease into 3 levels "High"/"Medium"/"Low" per a patient (1000 patients in all the data-set), determined by 23 various aspects of the patient (from age and gender to snoring or weight-loss level). The data is very clear and most of it numeric.

Then, we used PCA to understand the data better, but the results were not so good, which imply us that maybe there is problem with linear algorithm, we will back to this point.

After PCA, we tried to use other method ,KNN that does classification by clustering, we got great results of 100 100% accuracy using z-score normalization. We found that the best k is 1-9.  

The next algorithm was SVM, we wanted to use other classification method, that is not working on clustering, but on "learn-able" vector. The results were great 100% accuracy.

Because we got great results on KNN and SVM which separate the data non-linearly and because that on the PCA that works linearly we didn't get great results, we wanted to check if maybe algorithm that separate the data linearly is not suited to our data. As so we tried to use Linear SVM to explore this hypothesis, but Linear SVM worked great - 100% accuracy, which not support the hypothesis.

The next algorithm was DT, we wanted to use it for trying to understand better the importance of each feature. We found that 5 feature is good enough for great results in the DT - 100% accuracy. The feature per patient are - Passive Smoker, Fatigue, Wheezing, Obesity and snoring features level.

Then, we wanted to check again our hypothesis about not getting the best results on linear algorithm maybe because our data is not good enough for linear separation (??? it is not contradict to the fact that we checked Linear SVM and got great results???). As so, we deiced to use LDA. The results were good - 95% accuracy on the test data. So, in conclusion we understand that our data is excellent to separate and classify with non-linear algorithm (), but that the linear algorithm is good enough too(Linear SVM - 100%, LDA - 95 %). About our hypothesis we understand that the PCA is not good for our data.

After we analyzed and run different algorithms on our data we assume that we got very good and similar classification results for each algorithm, exclude PCA, with and without improvement maybe as a result of that small size of our data-set cause small group of test sampels which maybe was easy to classify. Another option is that our dataset is easy to classify duo to  differentiated  features. The algorithm we will choose from all of them is SVM. The reasons are - 
1. The results were excllent - 100% accuracy on linear and non-linear.
2. It is a "learn-able" algorithm, so the heavy learning part of the algorithm should run only once. If we want to take our model l further, we should train it on bigger dataset, in this case the learning part can take a while. After the learning will be done, we can use oyr algorthem to predict new case in much less costly runing time. another aspect of this is that we can pass our model without passing the training set, and use the model to predict new cases.
