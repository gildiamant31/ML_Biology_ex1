---
title: "ML_B final project - Cancer patients Dataset"
author: "Gil Diamant, Yishay Schlesinger & Itamar Twersky"
date: "17 05 2022"
output: pdf_document
---

```{r setup, global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.width=8, fig.height=3)
```

load libraries 

```{r}
set.seed(10)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots
library(ggplot2)      # load the ggplot2 library
library(readxl)
library(scales)
library(gmodels)
library(C50)
library(mlbench)
library(caret)
library(corrplot)
library(vip)          # for variable importance plots
library(klaR)
library(psych)
library(MASS)
library(devtools)
#install_github('fawda123/ggord')
library(ggord)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(purrr)
```

create themes for all prediction results plots:

```{r}
##### Theme Moma #####
theme_moma <- function(base_size = 12, base_family = "Helvetica") {
  theme(
    plot.background = element_rect(fill = "#F7F6ED"),
    legend.key = element_rect(fill = "#F7F6ED"),
    legend.background = element_rect(fill = "#F7F6ED"),
    panel.background = element_rect(fill = "#F7F6ED"),
    panel.border = element_rect(colour = "black", fill = NA, linetype = "dashed"),
    panel.grid.minor = element_line(colour = "#7F7F7F", linetype = "dotted"),
    panel.grid.major = element_line(colour = "#7F7F7F", linetype = "dotted")
  )
}
```

## Have a look at the data

Read the data to a local variable:
```{r}
cancer_patient <- read_excel("cancer patient data sets.xlsx")
```


Firstly, lets observe the properties of the data:

```{r}
#show data as str
str(cancer_patient)
#show data dim
dim(cancer_patient)
```
The data is about classified the level of the cancer disease for a patient, determined by 23 various aspects of the patient (from age and gender to snoring or weight loss level).

As we can see there is 3 cancer levels "High"/"Medium"/"Low" which are defined as a strings labels. For each patient there are various aspects
which are defined as integers 1-10 (exclude age(numeric),and gender(1-male/2-female)). Each patient is defined by id string.

From the table size we can see that we have 1000 patients.

Now, we want to check data clarity, and to check if there is NA/inf value.(if so, to "clean" the data).
Moreover, we want to check the classes balancing.

```{r}
any(is.na(cancer_patient))
total = c()
for (variable in cancer_patient) {
  total <- append(total, any(is.infinite(variable)))
}
any(total)
prop.table(table(cancer_patient$Level))
```

From the results, we can see that -
1. We don't have any NA/inf values, so the data clarity is fine.
2. The classes are balanced, so the data is almost evenly distributed, and we should preserve it when splitting the data.


Now, We are interested in creating a model with the ability to predict the level of the cancer for a patient, by its properties.

The level is string, and we wish to make it 'factor' for being categorical.

```{r}
# convert Type from numeric variable to factor (factor is an ordered object)
cancer_patient$Level<-as.factor(cancer_patient$Level)
```


Because our data contains many columns (23, the 1st is the patient_id and the 25th is the classification), we need a way to visualize most of the complexity represented in all dimensions in just two or three dimensions. Fortunately, that is exactly what dimension reduction does. Here, we apply a simple dimension reduction technique called principal component analysis (PCA) to all 23 numerical variables(the features) in the data set. 
* Important to mention that before doing any dimensional reduction, it is important to preprocess your data- in this case our data has only numeric values, as we shown before. 

The pca can be very useful in all of our analyzes for showing our data in 2D graph.

```{r}
# Preform PCA on our data
prcomp_cancer_patient <- prcomp(cancer_patient[, 2:24])
# bind between 2 first pcas and our dataframe
pca_cancer_patient <-  cbind(cancer_patient,PC1 = prcomp_cancer_patient$x[, 1],PC2 = prcomp_cancer_patient$x[, 2])
```

lets plot our pca results:
````{r}
# plot the types according two specific columns after PCA
ggplot(pca_cancer_patient, aes(y=PC1, x = PC2, col=Level)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("cancer patient Dataset")

```
pca gave as moderate results, lets check our variance - explaning in the 4 firsts pc components
```{r}
# variance of the 4 firsts pc components
print(round(prcomp_cancer_patient$sdev[1:4] / sum(prcomp_cancer_patient$sdev) * 100, 2))
```
as showed above - the pc components explain small part of the variance in the data, and this is probabaly the cause to the moderate success in the separation of the sampels.
????maybe that is because our data are not lineary seperable, and pca is a linear algorithem.????


## prepreing the data

Lets shuffle the data so it's order Will not affect our models 

```{r}
# shuffle the data by rows
#setting a seed for reproducible results
set.seed(1234)
pca_cancer_patient= pca_cancer_patient[sample(1:nrow(pca_cancer_patient)), ]
```

We saw above that the range of value is different between properties. ??? (all of them are 1-10)
Because we don't want that it will affect the importance of each property, we standardize the data with z-score.(to standard distribution)

```{r}
# z-score standardization (on the feuters columns only)
cancer_patient_z <- cbind(as.data.frame(scale(pca_cancer_patient[2:24])),pca_cancer_patient[25:27])

```


Note: because we are trying to classify 3 different levels of cancer, the only parameter matters is the accuracy, there is no recall..

We want to check if we can create better separation in ???non-linear??? algorithm, we chose KNN - clustering algorithm.

## KNN:
The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve classification problems.
The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.
KNN is looking for the 'k' nearest neighbors to the Unlabeled sample and classify it Like most neighbors, while 'k' is  a hyper-parameter we should tune.

lets try it on out data:
The next step is to create a training set and a test set, and to store the labels in a different vector to be used later. We divide our data into 80% train, 20% test.

```{r}
# create train set
cancer_train_z <- cancer_patient_z[1:800, ]
# create test set
cancer_test_z <- cancer_patient_z[801:1000, ]
# labels of train set
cancer_train_labels <- cancer_patient_z$Level[1:800]
# labels of test set
cancer_test_labels <- cancer_patient_z$Level[801:1000]

```

Lets check that the classes distribution is preserved between train and test:
```{r}
prop.table(table(cancer_train_labels))
prop.table(table(cancer_test_labels))
```
As we can see, the distribution is preserved.

load 'class' package for knn algorithm.
```{r}
library(class) 
```

And finally, we are ready to run our algorithm. We'll start with K=3 as arbitrary hyper-parameter, and check another options if its wont be good.

```{r}
# get predictions for the test set from knn, send as input the features columns and the train labels 
cancer_test_pred_z <- knn(train = cancer_train_z[1:23], test = cancer_test_z[1:23], cl = cancer_train_labels, k=3)
```

Since the knn function returns a factor vector of the predicted values, we'll compare that vector with the true labels we saved in advance. We'll do the comparison with the CrossTable function, from the gmodels package we'll load: 

```{r}
library(gmodels)
# show preformence in table (numeric)
CrossTable(x = cancer_test_labels, y = cancer_test_pred_z, prop.chisq=FALSE)

#knn_model contains only preicted value
test_v_1 <- cbind(cancer_test_z,cancer_test_pred_z)
#This will return TRUE or FALSE.
test_v_1$Correct <- test_v_1$Level == test_v_1$cancer_test_pred_z

# again here we also plot the types according two PCA with TRUE or FALSE labeling, and color of the true lable
ggplot(test_v_1, aes(y=PC1, x = PC2, col=cancer_test_labels, label = Correct)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("KNN Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
# True on each point means that it is correct prediction, FALSE means wrong prediction
```
In the above table we see that all of the patients were predicted correctly. ???REMOVE PCA???

Lets check the best K hyper-parameter for KNN -
```{r}
#setting a seed for reproducible results
set.seed(1234)
# loop over 40 different k values and saved the accuracy of algorithm prediction in array
i=1
k_options=1
for (i in 1:40){
knn_pred <- knn(train=cancer_train_z[1:23], test=cancer_test_z[1:23], cl=cancer_train_labels, k=i)
k_options[i] <- 100 * sum(cancer_test_labels == knn_pred)/NROW(cancer_test_labels)
}


# show the differences of the accuracy for each k value in a plot.
plot(k_options, type="b", xlab="K- Value",ylab="Accuracy level", main = "knn model accuracy for different k values")
```

As we saw, we can use KNN to as multi-class classifier algorithm.

If we use 'k' in values of 1 - 9 then the KNN got excellent results with zero mistakes.

Now, we want to check another classification algorithm. but this time we want to check other approach. The KNN working by clustering, and now we want to work by a "learnable" vector separation model, so we will use SVM.

##  SVM:
A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes.
SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes.
SVMs are more commonly used in classification problems and as such, we will use it here on our level of cancer classification.

we will start with c=0.1 and gamma = 10 as arbitrary hyper-parameters -

```{r}
# fit model

classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "radial", cost = 0.1, gamma = 10)
print(classifier)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])

library(gmodels)
# show preformence in table
CrossTable(x = cancer_test_labels, y = svm_pred, prop.chisq=FALSE)

#knn_model contains only preicted value
test_v_1 <- cbind(cancer_test_z,svm_pred)
#This will return TRUE or FALSE.
test_v_1$Correct <- test_v_1$Level == test_v_1$svm_pred



# again here we also plot the types according two PCA with TRUE or FALSE labeling, and color of the true lable
ggplot(test_v_1, aes(y=PC1, x = PC2, col=cancer_test_labels, label = Correct, shape = Level)) +
  geom_point(size=5, alpha = 0.2) + 
  scale_color_manual(values=c("#000000","#FF0000","#00BA00")) +
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("SVM Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
```
We got 3 mistakes on the test, so now we want to find better hyper-parameters.

# tune cost and gama hyper prameters
```{r}
# tune model to find optimal cost and gamma values
tune.out <- tune(svm, Level~ ., data = cancer_train_z[1:24], kernel = "radial",ranges = list(gamma = c(0.1, 1, 5, 10, 100), cost = c(0.1, 1, 5, 10, 100)),tunecontrol = tune.control(sampling = "fix"))
# show best model
print(tune.out$best.parameters)
```

???I cant see the above output....???
From the results we can see the c=0.1 and gamma=0.1 are better hyper-parameter, so lets run with them -
```{r}
# fit model
classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "radial", cost = 0.1, gamma = 0.1)
print(classifier)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])

library(gmodels)
# show preference in table
CrossTable(x = cancer_test_labels, y = svm_pred, prop.chisq=FALSE)
```
Now we have 0 mistakes, so we got excellent results with SVM.


??? Maybe it is not needed???
We saw very great results in KNN and radial SVM, but moderate separation in the PCA(although PCA is not classifier, we expect to see good separation of the samples by class which is not so good). We suggest that it may cause by the data which is cannot be separated linearly, so we want to test linear SVM to try to approve our hypothesis.

# linear svm

```{r}
# fit model
classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "linear", cost = 0.1, gamma = 0.1)
print(classifier)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])

library(gmodels)
# show preformence in table
CrossTable(x = cancer_test_labels, y = svm_pred, prop.chisq=FALSE)
```
From the results we can see that we have 0 mistakes, which not support our hypothesis.

Now, After KNN and SVM, we wanted to test a model that gives us information about patient's properties importance for the level of cancer, which we can learn from the output of the model.
We choose Decision-Tree model for our next model -

##  Decision-Tree classifer:


Decision Tree algorithm belongs to the family of supervised learning algorithms. The decision tree algorithm can be used for solving regression and classification problems too.
The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).
In Decision Trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with the record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.

we already have training and test sets, normalized and shuffled,
lets run our DT

```{r}
#setting a seed for reproducible results
set.seed(1234)
# apply model in training data (8th column is the label to be predicted)
cancer_model <- C5.0(cancer_train_z[1:23], cancer_train_labels)
#take a look at our model
cancer_model %>% vip()
```
We can see that our tree relay hard on only 5 of the features and maybe its decrease our performance - lets check the accuracy.

Next we'll look at the summary of the model.

```{r}
summary(cancer_model)
```
We got 100% accuracy ???(on train data)???.

So Another way to look about our tree relay hard on only 5 of the features, is that we can see that we got 100% accuracy relaying on those features so we can said that the patient's level of the cancer can be predicted by its Passive Smoker, Fatigue, Wheezing, Obesity and snoring features level, and it is not decrease our performance as we thought before.

As we now know, it is very important to evaluate our model performance:

```{r}
#setting a seed for reproducible results
set.seed(1234)
# apply model on test data
cancer_pred <- predict(cancer_model, cancer_test_z[1:23])

CrossTable(cancer_test_labels, cancer_pred, prop.chisq = FALSE,
           prop.c = FALSE, prop.r = FALSE, 
           dnn = c('actual Type', 'predicted Type'))

#DT_Model merge
#pred contains only predicted value
test_merged1 <- cbind(cancer_test_z,cancer_pred)
test_merged1$Correct <- test_merged1$Level == 
  test_merged1$cancer_pred #This will return TRUE or FALSE.

# again here we also plot the types according two specific columns after PCA
c1 <-ggplot(test_merged1, aes(y=PC1, x = PC2, col=Level, label = Correct)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("DT Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
show(c1)
```

From the results we can see the we have 100% accuracy.

Now we want to try and explore again if we can separate our data in linear way, but other than PCA. So we chose to use LDA, that focusing more on separation rather than variation (like PCA). Moreover, 


## LDA
Linear discriminant analysis (LDA), is used as a tool for classification, dimension reduction, and data visualization. It is try to maximize the distance between the means and to minimize the variation.

Lets run LDA -
```{r}
# apply LDA
linear <- lda(Level~., cancer_train_z[1:24])
#  calculate lda poprportins
lda_porpotions = signif(prop.table(linear$svd^2),3)
# take a look to see how the ldas explaining our data:
print("LDA1   LDA2")
print( paste0(as.character(lda_porpotions[1]),"  ", as.character(lda_porpotions[2])))
```

The LDA1 is high (0.888) which show that it has the best variation between categories that LDA create, and LDA2 is seconed one. Both of them are the vast majority to explain the variation between the categories for us (in contrast to the PCA1 and PCA2 we saw before). 

Lets view the diffrence between the seperation of LDA1 and LDA2 - 
```{r}
# Histogram using LDA1
p <- predict(linear, cancer_train_z[1:24])
ldahist(data = p$x[,1], g = cancer_train_z$Level)
ldahist(data = p$x[,2], g = cancer_train_z$Level)
```
We can see that the separation is not perfect -
1. In LD1 there is overlap between Medium and Low.
2. In LD2 there is overlap between all of the classes (Low,Medium,High).
NOTE: the separation in LDA1 much better corresponding to the results before.

### bi-plot
Lets view the LDA1/2 plot -
```{r}
ggord::ggord(linear, cancer_train_z$Level, ylim = c(-5, 5),txt = NULL)
```
From the plot we can see very good separation, although its not perfect, which corresponding to the results above.

### Confusion matrix and accuracy – training data
Lets get the classification on the data, start with the 'train' - 
```{r}
# prediction - training data
p1 <- predict(linear, cancer_train_z[1:23])$class
tab <- table(Predicted = p1, Actual = cancer_train_z$Level)
tab

sum(diag(tab))/sum(tab)
```
From the results we can see that we have 32 mistakes, 31 of them are Medium that classified as Low, which corresponding to the plot above (see the green(Low) and the blue(Medium) circles). As so, the accuracy 96%, good but not perfect.


### Confusion matrix and accuracy – testing data
Lets get the classification on the 'test' data - 

```{r}
# prediction - test data
p2 <- predict(linear,  cancer_test_z[1:23])$class
tab1 <- table(Predicted = p2, Actual = cancer_test_z$Level)
tab1

sum(diag(tab1))/sum(tab1)
```
We can see that on the test we have 10 mistakes, again Medium that classified as Low, which corresponding to the plot above and to the train classification by the LDA. The accuracy is 95%,good but not perfect, corresponding to the train.

Those results can imply us that the linear algorithm is not the best for our data.

???THERE is needed in the TEST and TRAIN in the LDA???

## Summary & Conclusions :

firstly, we analyze the data, and understood it. The data is about classified the cancer disease into 3 levels "High"/"Medium"/"Low" per a patient (1000 patients in all the data-set), determined by 23 various aspects of the patient (from age and gender to snoring or weight-loss level). The data is very clear and most of it numeric.

Then, we used PCA to understand the data better, but the results were not so good, which imply us that maybe there is problem with linear algorithm, we will back to this point.

After PCA, we tried to use other method ,KNN that does classification by clustering, we got great results of 100 100% accuracy using z-score normalization. We found that the best k is 1-9.  

The next algorithm was SVM, we wanted to use other classification method, that is not working on clustering, but on "learn-able" vector. The results were great 100% accuracy.

Because we got great results on KNN and SVM which separate the data non-linearly and because that on the PCA that works linearly we didnt get great results, we wanted to check if maybe algorithm that separate the data linearly is not suited to our data. As so we tried to use Linear SVM to explore this hypothesis, but Linear SVM worked great - 100% accuracy, which not support the hypothesis, but we will try and check it agian later.

The next algorithm was DT, we wanted to use it for trying to understand better the importance of each feature. We found that 5 feature is good enough for great results in the DT - 100% accuracy. The feature per patient are - Passive Smoker, Fatigue, Wheezing, Obesity and snoring features level.

Then, we wanted to check again our hypothesis about not getting the best results on linear algorithm maybe because our data is not good enough for linear separation (??? it is not contradict to the fact that we checked Linear SVM and got great results???). As so, we deiced to use LDA. The results were good - 95% accuracy on the test data. So, in conclusion we understand that our data is excellent to separate and classify with non-linear algorithm (), but that the linear algorithm is good enough too(Linear SVM - 100%, LDA - 95 %). About our hypothesis we understand that the PCA is not good for our data.

After we analyzed and run different algorithms on our data we assume that we got very good and similar classification results for each algorithm, exclude PCA, with and without improvement maybe as a result of that small size of our data-set cause small group of test sampels which maybe was easy to classify. Another option is that our dataset is easy to classify duo to  differentiated  features. The algorithm we will choose from all of them is SVM. The reasons are - 
1. The results were excllent - 100% accuracy on linear and non-linear.
2. It is a "learn-able" algorithm, so we assume it will be better for future data that someone will add to the data-set.
