---
title: "ML_B final project - Cancer patients Dataset"
author: "Gil Diamant, Yishay Schlesinger & Itamar Twersky"
date: "17 05 2022"
output: pdf_document
---

```{r setup, global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.width=8, fig.height=3)
```

load libraries 

```{r}
set.seed(10)
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots
library(ggplot2)      # load the ggplot2 library
library(readxl)
library(scales)
library(gmodels)
library(C50)
library(mlbench)
library(caret)
library(corrplot)
library(vip)          # for variable importance plots
library(klaR)
library(psych)
library(MASS)
library(devtools)
#install_github('fawda123/ggord')
library(ggord)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(purrr)
```

create themes for all prediction results plots:

```{r}
##### Theme Moma #####
theme_moma <- function(base_size = 12, base_family = "Helvetica") {
  theme(
    plot.background = element_rect(fill = "#F7F6ED"),
    legend.key = element_rect(fill = "#F7F6ED"),
    legend.background = element_rect(fill = "#F7F6ED"),
    panel.background = element_rect(fill = "#F7F6ED"),
    panel.border = element_rect(colour = "black", fill = NA, linetype = "dashed"),
    panel.grid.minor = element_line(colour = "#7F7F7F", linetype = "dotted"),
    panel.grid.major = element_line(colour = "#7F7F7F", linetype = "dotted")
  )
}
```

## Have a look at the data

Read the data to a local variable:
```{r}
cancer_patient <- read_excel("cancer patient data sets.xlsx")
```


Firstly, lets observe the properties of the data:

```{r}
#show data as str
str(cancer_patient)
#show data dim
dim(cancer_patient)
```
The data is about classified the level of the cancer disease for a patient, determined by 23 various aspects of the patient (from age and gender to snoring or weight loss level).

As we can see there is 3 cancer levels "High"/"Medium"/"Low" which are defined as a strings labels. For each patient there are various aspects
which are defined as integers 1-10 (exclude age(numeric),and gender(1-male/2-female)). Each patient is defined by id string.

From the table size we can see that we have 1000 patients.

Now, we want to check data clarity, and to check if there is NA/inf value.(if so, to "clean" the data).
Moreover, we want to check the classes balancing.

```{r}
any(is.na(cancer_patient))
total = c()
for (variable in cancer_patient) {
  total <- append(total, any(is.infinite(variable)))
}
any(total)
prop.table(table(cancer_patient$Level))
```

From the results, we can see that -
1. We don't have any NA/inf values, so the data clarity is fine.
2. The classes are balanced, so the data is almost evenly distributed, and we should preserve it when splitting the data.


Now, We are interested in creating a model with the ability to predict the level of the cancer for a patient, by its properties.

The level is string, and we wish to make it 'factor' for being categorical.

```{r}
# convert Type from numeric variable to factor (factor is an ordered object)
cancer_patient$Level<-as.factor(cancer_patient$Level)
```


Because our data contains many columns (23, the 1st is the patient_id and the 25th is the classification), we need a way to visualize most of the complexity represented in all dimensions in just two or three dimensions. Fortunately, that is exactly what dimension reduction does. Here, we apply a simple dimension reduction technique called principal component analysis (PCA) to all 23 numerical variables(the features) in the data set. 
* Important to mention that before doing any dimensional reduction, it is important to preprocess your data- in this case our data has only numeric values, as we shown before. 

The pca can be very useful in all of our analyzes for showing our data in 2D graph.

```{r}
# Preform PCA on our data
prcomp_cancer_patient <- prcomp(cancer_patient[, 2:24])
# bind between 2 first pcas and our dataframe
pca_cancer_patient <-  cbind(cancer_patient,PC1 = prcomp_cancer_patient$x[, 1],PC2 = prcomp_cancer_patient$x[, 2])
```

lets plot our pca results:
````{r}
# plot the types according two specific columns after PCA
ggplot(pca_cancer_patient, aes(y=PC1, x = PC2, col=Level)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("cancer patient Dataset")

```
pca gave as moderate results, lets check our variance - explaning in the 4 firsts pc components
```{r}
# variance of the 4 firsts pc components
print(round(prcomp_cancer_patient$sdev[1:4] / sum(prcomp_cancer_patient$sdev) * 100, 2))
```
as showed above - the pc components explain small part of the variance in the data, and this is probabaly the cause to the moderate success in the separation of the sampels.
????maybe that is because our data are not lineary seperable, and pca is a linear algorithem.????


## prepreing the data

Lets shuffle the data so it's order Will not affect our models 

```{r}
# shuffle the data by rows
#setting a seed for reproducible results
set.seed(1234)
pca_cancer_patient= pca_cancer_patient[sample(1:nrow(pca_cancer_patient)), ]
```

We saw above that the range of value is different between properties. ??? (all of them are 1-10)
Because we don't want that it will affect the importance of each property, we standardize the data with z-score.(to standard distribution)

```{r}
# z-score standardization (on the feuters columns only)
cancer_patient_z <- cbind(as.data.frame(scale(pca_cancer_patient[2:24])),pca_cancer_patient[25:27])

```


Note: because we are trying to classify 3 different levels of cancer, the only parameter matters is the accuracy, there is no recall..

We want to check if we can create better separation in ???non-linear??? algorithm, we chose KNN - clustering algorithm.

## KNN:
The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve classification problems.
The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.
KNN is looking for the 'k' nearest neighbors to the Unlabeled sample and classify it Like most neighbors, while 'k' is  a hyper-parameter we should tune.

lets try it on out data:
The next step is to create a training set and a test set, and to store the labels in a different vector to be used later. We divide our data into 80% train, 20% test.

```{r}
# create train set
cancer_train_z <- cancer_patient_z[1:800, ]
# create test set
cancer_test_z <- cancer_patient_z[801:1000, ]
# labels of train set
cancer_train_labels <- cancer_patient_z$Level[1:800]
# labels of test set
cancer_test_labels <- cancer_patient_z$Level[801:1000]

```

Lets check that the classes distribution is preserved between train and test:
```{r}
prop.table(table(cancer_train_labels))
prop.table(table(cancer_test_labels))
```
As we can see, the distribution is preserved.

load 'class' package for knn algorithm.
```{r}
library(class) 
```

And finally, we are ready to run our algorithm. We'll start with K=3 as arbitrary hyper-parameter, and check another options if its wont be good.

```{r}
# get predictions for the test set from knn, send as input the features columns and the train labels 
cancer_test_pred_z <- knn(train = cancer_train_z[1:23], test = cancer_test_z[1:23], cl = cancer_train_labels, k=3)
```

Since the knn function returns a factor vector of the predicted values, we'll compare that vector with the true labels we saved in advance. We'll do the comparison with the CrossTable function, from the gmodels package we'll load: 

```{r}
library(gmodels)
# show preformence in table (numeric)
CrossTable(x = cancer_test_labels, y = cancer_test_pred_z, prop.chisq=FALSE)

#knn_model contains only preicted value
test_v_1 <- cbind(cancer_test_z,cancer_test_pred_z)
#This will return TRUE or FALSE.
test_v_1$Correct <- test_v_1$Level == test_v_1$cancer_test_pred_z

# again here we also plot the types according two PCA with TRUE or FALSE labeling, and color of the true lable
ggplot(test_v_1, aes(y=PC1, x = PC2, col=cancer_test_labels, label = Correct)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("KNN Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
# True on each point means that it is correct prediction, FALSE means wrong prediction
```
In the above table we see that all of the patients were predicted correctly. ???REMOVE PCA???

Lets check the best K hyper-parameter for KNN -
```{r}
#setting a seed for reproducible results
set.seed(1234)
# loop over 40 different k values and saved the accuracy of algorithm prediction in array
i=1
k_options=1
for (i in 1:40){
knn_pred <- knn(train=cancer_train_z[1:23], test=cancer_test_z[1:23], cl=cancer_train_labels, k=i)
k_options[i] <- 100 * sum(cancer_test_labels == knn_pred)/NROW(cancer_test_labels)
}


# show the differences of the accuracy for each k value in a plot.
plot(k_options, type="b", xlab="K- Value",ylab="Accuracy level", main = "knn model accuracy for different k values")
```

As we saw, we can use KNN to as multi-class classifier algorithm.

If we use 'k' in values of 1 - 9 then the KNN got excellent results with zero mistakes.

Now, we want to check another classification algorithm. but this time we want to check other approach. The KNN working by clustering, and now we want to work by a "learnable" vector separation model, so we will use SVM.

##  SVM:
A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes.
SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes.
SVMs are more commonly used in classification problems and as such, we will use it here on our level of cancer classification.

we will start with c=0.1 and gamma = 10 as arbitrary hyper-parameters -

```{r}
# fit model

classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "radial", cost = 0.1, gamma = 10)
print(classifier)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])

library(gmodels)
# show preformence in table
CrossTable(x = cancer_test_labels, y = svm_pred, prop.chisq=FALSE)

#knn_model contains only preicted value
test_v_1 <- cbind(cancer_test_z,svm_pred)
#This will return TRUE or FALSE.
test_v_1$Correct <- test_v_1$Level == test_v_1$svm_pred



# again here we also plot the types according two PCA with TRUE or FALSE labeling, and color of the true lable
ggplot(test_v_1, aes(y=PC1, x = PC2, col=cancer_test_labels, label = Correct, shape = Level)) +
  geom_point(size=5, alpha = 0.2) + 
  scale_color_manual(values=c("#000000","#FF0000","#00BA00")) +
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("SVM Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
```
We got 3 mistakes on the test, so now we want to find better hyper-parameters.

# tune cost and gama hyper prameters
```{r}
# tune model to find optimal cost and gamma values
tune.out <- tune(svm, Level~ ., data = cancer_train_z[1:24], kernel = "radial",ranges = list(gamma = c(0.1, 1, 5, 10, 100), cost = c(0.1, 1, 5, 10, 100)),tunecontrol = tune.control(sampling = "fix"))
# show best model
print(tune.out$best.parameters)
```

???I cant see the above output....???
From the results we can see the c=0.1 and gamma=0.1 are better hyper-parameter, so lets run with them -
```{r}
# fit model
classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "radial", cost = 0.1, gamma = 0.1)
print(classifier)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])

library(gmodels)
# show preference in table
CrossTable(x = cancer_test_labels, y = svm_pred, prop.chisq=FALSE)
```
Now we have 0 mistakes, so we got excellent results with SVM.


??? Maybe it is not needed???
We saw very great results in KNN and radial SVM, but moderate separation in the PCA(although PCA is not classifier, we expect to see good separation of the samples by class which is not so good). We suggest that it may cause by the data which is cannot be separated linearly, so we want to test linear SVM to try to approve our hypothesis.

# linear svm

```{r}
# fit model
classifier <- svm(Level~ ., data = cancer_train_z[1:24], kernel = "linear", cost = 0.1, gamma = 0.1)
print(classifier)
svm_pred = predict(classifier, newdata = cancer_test_z[1:23])

library(gmodels)
# show preformence in table
CrossTable(x = cancer_test_labels, y = svm_pred, prop.chisq=FALSE)
```
From the results we can see that we have 0 mistakes, which not support our hypothesis.

Now, After KNN and SVM, we wanted to test a model that gives us information about patient's properties importance for the level of cancer, which we can learn from the output of the model.
We choose Decision-Tree model for our next model -

##  Decision-Tree classifer:


Decision Tree algorithm belongs to the family of supervised learning algorithms. The decision tree algorithm can be used for solving regression and classification problems too.
The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).
In Decision Trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with the record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.

we already have training and test sets, normalized and shuffled,
lets run our DT

```{r}
#setting a seed for reproducible results
set.seed(1234)
# apply model in training data (8th column is the label to be predicted)
cancer_model <- C5.0(cancer_train_z[1:23], cancer_train_labels)
#take a look at our model
cancer_model %>% vip()
```
We can see that our tree relay hard on only 5 of the features and maybe its decrease our performance - lets check the accuracy.

Next we'll look at the summary of the model.

```{r}
summary(cancer_model)
```
We got 100% accuracy ???(on train data)???.

So Another way to look about our tree relay hard on only 5 of the features, is that we can see that we got 100% accuracy relaying on those features so we can said that the patient's level of the cancer can be predicted by its Passive Smoker, Fatigue, Wheezing, Obesity and snoring features level, and it is not decrease our performance as we thought before.

As we now know, it is very important to evaluate our model performance:

```{r}
#setting a seed for reproducible results
set.seed(1234)
# apply model on test data
cancer_pred <- predict(cancer_model, cancer_test_z[1:23])

CrossTable(cancer_test_labels, cancer_pred, prop.chisq = FALSE,
           prop.c = FALSE, prop.r = FALSE, 
           dnn = c('actual Type', 'predicted Type'))

#DT_Model merge
#pred contains only predicted value
test_merged1 <- cbind(cancer_test_z,cancer_pred)
test_merged1$Correct <- test_merged1$Level == 
  test_merged1$cancer_pred #This will return TRUE or FALSE.

# again here we also plot the types according two specific columns after PCA
c1 <-ggplot(test_merged1, aes(y=PC1, x = PC2, col=Level, label = Correct)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("DT Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
show(c1)
```

From the results we can see the we have 100% accuracy.

Now we want to try and explore again if we can separate our data in linear way, but other than PCA. So we chose to use LDA, that focusing more on separation rather variation (like PCA).


## LDA
Linear discriminant analysis (LDA), is used as a tool for classification, dimension reduction, and data visualization. It is try to maximize the distance between the means and to minimize the variation.

Lets run LDA -
```{r}
# apply LDA
linear <- lda(Level~., cancer_train_z[1:24])
#  calculate lda poprportins
lda_porpotions = signif(prop.table(linear$svd^2),3)
# take a look to see how the ldas explaning our data:
print("LDA1   LDA2")
print( paste0(as.character(lda_porpotions[1]),"  ", as.character(lda_porpotions[2])))
```


```{r}
# Historgram using LDA1
p <- predict(linear, cancer_train_z[1:24])
ldahist(data = p$x[,1], g = cancer_train_z$Level)
ldahist(data = p$x[,2], g = cancer_train_z$Level)
```
we can see not perfect separation in LD1 but we also can see the range of Low -> Medium -> High

### bi-plot

```{r}
ggord::ggord(linear, cancer_train_z$Level, ylim = c(-5, 5),txt = NULL)
```
### Confusion matrix and accuracy – training data

```{r}
# prediction - training data
p1 <- predict(linear, cancer_train_z[1:23])$class
tab <- table(Predicted = p1, Actual = cancer_train_z$Level)
tab

sum(diag(tab))/sum(tab)
```
### Confusion matrix and accuracy – testing data

```{r}
# prediction - test data
p2 <- predict(linear,  cancer_test_z[1:23])$class
tab1 <- table(Predicted = p2, Actual = cancer_test_z$Level)
tab1

sum(diag(tab1))/sum(tab1)
```


##### work until here !!!!!##############################################

# TODO fix from here to the end. maybe remove all of it and write some comparsion

### Comparing between the two models:

compare the best models prediction results of both algorithms,
This time we used colors to emphsize mistakes:
```{r}
#KNN_Model merge

#knn_model contains only preicted value
test_v_1 <- cbind(cancer_n_test,knn_pred_k5)
#This will return TRUE or FALSE.
test_v_1$Correct <- test_v_1$Level == test_v_1$knn_pred_k5 

# again here we also plot the types according two specific columns after PCA
c1 <-ggplot(test_v_1, aes(y=PC1, x = PC2, col=Correct,shape = Level ,label = Correct)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("KNN Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
c2 <-ggplot(test_merged2, aes(y=PC1, x = PC2, col=Correct,shape = Level ,label = Correct)) +
  geom_point(size=5, alpha = 0.2) + 
  theme_moma() +
  theme(legend.position = "bottom") +
  ggtitle("Improved DT Prediction Results") + 
  geom_text(size = 3, show.legend = FALSE)
# plot both in one line
grid.arrange(c1,c2, nrow = 1)
# True on each point means that it is correct prediction, FALSE means wrong prediction
```
We can see both of them are mistaken in the same sample, which look like its very hard to classify because its on the border.
The DT also have anothe mistake which the knn are preventing from.


compare the accuracy of each algorithm best model:
```{r}
# get accuracy of DT boost prediction
DT_acc <- 100 * sum(cancer_test_labels == cancer_boost_pred10)/NROW(cancer_test_labels)
# get accuracy of knn_k5 prediction
KNN_acc <- 100 * sum(cancer_test_labels == knn_pred_k5)/NROW(cancer_test_labels)

# create data frame with the accuracy to plot it.
Algo <- c("DT","KNN")
Accuracy <- c(DT_acc,KNN_acc)
df <- data.frame(Algo, Accuracy)
ggplot(data = df ,mapping = aes(x=Algo, y= Accuracy, fill = Accuracy)) +
  geom_col() + 
  labs(title = "Compare DT accuracy to KNN accuracy",
  y = "Accuracy", x = "Algorithm")
```

## Summary & Conclusions :

firstly, we  tried KNN with k=2 and we got nice results of 92% accuracy using Z-score normalization.
After checking big range of 'k' , the best prediction result was over 96% accuracy in predictions, while using hyper-parameter K=5(while using min-max normalization).

The next algorithm was DT we use it on our dataset after normalization process(min-max) without improvement and got 92% accuracy.
After aggregating many decision trees by boosting improvement (10 trails of the algorithm) on DT algorithm we improved to 94% accuracy in algo's predictions.
We used the default Decision Tree and its default hyper-parameters.

After comparing two algorithms results (see the two comparing plots) we assume that KNN with K=5 as hyper-parameter is the the better algorithm to choose while classifing this data. However, the KNN is 'dummmy' algorithrm which dose'nt remember which means its has to go over all the training set for every classification, which is costly in runing time. The DT need some time for its train but the classification is lighter. So in case of need in fast classification, we reccomend chossing the DT with boosting of 10 trails which also gave very good preformance .


After we analyzed and run different algorithms on our data we assume that we got very good and similar classification results for each algorithm with and without improvement maybe as a result of that small size of our dataset cause small group of test sampels which maybe was easy to classify. Another option is that our dataset is easy to classify dou to  differentiated  features.

As said before, our results are hardly connected with our test set - which is very small and have great impact on our results.
To validate our results its will be clever to rerun our analyzes several time with different test sets and see if the results stay as is.
