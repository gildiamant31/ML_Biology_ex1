---
title: "Tirgul 2 - Dimensionality Reduction"
author: "Romi Goldner Kabeli"
date: "4/13/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(ggplot2)
library(ggrepel)
library(cowplot)
library(GGally)
```

We will work on cereal data:

```{r}
cereals <- read.csv("cereal.csv", header = TRUE, row.names = 1)
str(cereals)
```

Looks like we have 77 brands of cereal with 12 variables describing them, which is a lot. 
Lets leave only the numerical features:

```{r}
numeric_vars <- c(
  "calories", "protein", "fat", "sodium", "fiber", "carbo",
  "sugars", "potass", "vitamins", "rating"
)

cereals_num <- cereals[, numeric_vars]
ncol(cereals_num)
```

Now, lets say we want to categorize these cereals. We'll call some the "fun" ones, and some the "shredded" ones

```{r}
fun_cereals <- c(
  "Lucky_Charms", "Count_Chocula", "Froot_Loops",
  "Frosted_Flakes", "Cocoa_Puffs", "Cinnamon_Toast_Crunch"
)

shredded_wheat_cereals <- c(
  "Shredded_Wheat", "Shredded_Wheat_'n'Bran",
  "Shredded_Wheat_spoon_size"
)

cereals_num_sub <- match(
  c(fun_cereals, shredded_wheat_cereals),
  rownames(cereals_num)
)

cereals_num$classification <- "normal"
cereals_num$classification[match(fun_cereals, rownames(cereals_num))] <- "fun"
cereals_num$classification[match(
  shredded_wheat_cereals,
  rownames(cereals_num)
)] <- "shredded"

cereals_num$label <- ""
cereals_num$label[cereals_num_sub] <- rownames(cereals_num)[cereals_num_sub]
```

Now, we can think of some questions to ask about our data:

Are all fun cereals the same? what about the shredded ones?
Can some cereals not currently classified be added to any category?
How different are the cereals in each category?


A quick look at our data is still pretty confusing:

```{r}
ggpairs(cereals_num,
  columns = c("fat", "calories", "sodium", "sugars"),
  ggplot2::aes(colour = classification)
)
```

Because our data contains many columns, we need a way to visualize most of the complexity represented in all dimensions in just two or three dimensions. Fortunately, that is exactly what dimension reduction does. Here, we apply a simple dimension reduction technique called principal component analysis (PCA) to all 10 numerical variables in the data set. 
* Important to mention that before doing any dimensionality reduction, it is important to preprocess your data. 

```{r}
# Preform PCA on our data
prcomp_cereals_num <- prcomp(cereals_num[, 1:10])
pca_cereals_num <- data.frame(
  PC1 = prcomp_cereals_num$x[, 1],
  PC2 = prcomp_cereals_num$x[, 2],
  label = cereals_num$label,
  classification = cereals_num$classification
)

ggplot(pca_cereals_num, aes(x = PC1, y = PC2, label = label, col = classification)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5)
```

Using the PCA, we can now easily find cereals that are similar to some of the fun cereals. These will have similar values of the principal components as the fun cereals. Let's find out who they are:


```{r}
cereals_num_int <- which(pca_cereals_num$PC1 > 0 &
  pca_cereals_num$PC1 < 75 &
  pca_cereals_num$PC2 > 25)

pca_cereals_num$label2 <- ""
pca_cereals_num$label2[cereals_num_int] <- rownames(cereals_num)[cereals_num_int]
pca_cereals_num$label2[cereals_num_sub] <- ""

ggplot(pca_cereals_num, aes(x = PC1, y = PC2, label = label2, col = classification)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5)
```

According to the first two principal components other fun cereals could be Honeycomb, Cap’n’Crunch and Crispix. Notice that even the packaging of these cereals is much more similar to the packaging of the fun cereals than the shredded cereals.

# Some more dimensionality reduction techniques

## MDS

Multidimensional scaling (MDS) is an algorithm that given a distance matrix with the distances between each pair of objects in a set, and a chosen number of dimensions, N, places each object into N-dimensional space such that the between-object distances are preserved as well as possible. MDS is most often used as a visualization tool. It is best suited to problems that involve real distances, i.e. city distances or geometry. It yields the same result as PCA when Euclidean distances are used.

```{r}
dist_cereals_num <- dist(cereals_num[, 1:12], method = "canberra")
mds_cereals_num <- cmdscale(dist_cereals_num, eig = TRUE, k = 2)

mds_cereals_num <- data.frame(
  MDS1 = mds_cereals_num$points[, 1],
  MDS2 = mds_cereals_num$points[, 2],
  label = cereals_num$label,
  classification = cereals_num$classification
)

ggplot(mds_cereals_num, aes(
  x = MDS1, y = MDS2,
  label = label, col = classification
)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5)
```

## TSNE

T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimension reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability. The main application of t-SNE is for visualizations of large amount of data, as it can recover well-separated clusters.

```{r}
library(Rtsne)
tsne_cereals_num <- Rtsne(cereals_num[, 1:12],
  pca = FALSE, perplexity = 10,
  theta = 0.0
)

tsne_cereals_num <- data.frame(
  TSNE1 = tsne_cereals_num$Y[, 1],
  TSNE2 = tsne_cereals_num$Y[, 2],
  label = cereals_num$label,
  classification = cereals_num$classification
)

ggplot(tsne_cereals_num, aes(
  x = TSNE1, y = TSNE2,
  label = label, col = classification
)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5)
```

## UMAP

Uniform Manifold Approximation and Projection (UMAP) produce a low dimensional representation of high dimensional data that preserves relevant structure. It searches for a low dimensional projection of the data that has the closest possible equivalent topological structure to the original high dimensional data. UMAP is similar to t-SNE but preserves more global structure in the data. Its main use is visualization of a large amount of observations as it recovers well-separated clusters.

```{r}
library(uwot)
umap_cereals_num <- umap(cereals_num[, 1:12],
  n_neighbors = 15,
  min_dist = 1, spread = 5
)

umap_cereals_num <- data.frame(
  UMAP1 = umap_cereals_num[, 1],
  UMAP2 = umap_cereals_num[, 2],
  label = cereals_num$label,
  classification = cereals_num$classification
)

ggplot(umap_cereals_num, aes(
  x = UMAP1, y = UMAP2,
  label = label, col = classification
)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5)
```

Compare UMAP results to the categorized cereals:

```{r}
cereals_num_int <- which(umap_cereals_num$UMAP1 < 2 &
  umap_cereals_num$UMAP1 > -10 &
  umap_cereals_num$UMAP2 > 2 &
  umap_cereals_num$UMAP2 < 12)

umap_cereals_num$label2 <- ""
umap_cereals_num$label2[cereals_num_int] <- rownames(cereals_num)[cereals_num_int]
umap_cereals_num$label2[cereals_num_sub] <- ""

ggplot(umap_cereals_num, aes(
  x = UMAP1, y = UMAP2,
  label = label2, col = classification
)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5)
```

## Number of dimensions to retain

So far we have really only looked at the first two dimensions obtained after dimension reduction. However, this was merely a choice. In fact dimensions beyond the first two, often contain vital information. One way to check this is to look at the variance explained by each principle component. In fact it is good practice when using PCA to state the explained variance by every principle component plotted.

```{r}
prcomp_cereals_num <- prcomp(cereals_num[, 1:10])
var_exp <- round(prcomp_cereals_num$sdev[1:4] / sum(prcomp_cereals_num$sdev) * 100, 2)
pca_cereals_num <- data.frame(
  PC1 = prcomp_cereals_num$x[, 1],
  PC2 = prcomp_cereals_num$x[, 2],
  PC3 = prcomp_cereals_num$x[, 3],
  PC4 = prcomp_cereals_num$x[, 4],
  label = cereals_num$label,
  classification = cereals_num$classification
)

comb <- list(
  first = c(1, 2), second = c(1, 3), third = c(1, 4), fourth = c(2, 3),
  fifth = c(2, 4), sixth = c(3, 4)
)

gg <- lapply(comb, function(x) {
  ggplot(pca_cereals_num, aes_string(
    x = paste0("PC", x[1]),
    y = paste0("PC", x[2]), col = "classification"
  )) +
    geom_point() + guides(col = FALSE) + ylab(paste0(
      "PC", x[2], ":(",
      var_exp[x[2]], "%)"
    )) + xlab(paste0("PC", x[1], ":(", var_exp[x[1]], "%)"))
})

plot_grid(plotlist = gg)
```


```{r}
var_exp <- data.frame(
  "Variance explained" =
    prcomp_cereals_num$sdev / sum(prcomp_cereals_num$sdev) * 100,
  "Number of dimension" = 1:length(prcomp_cereals_num$sde),
  check.names = FALSE
)

ggplot(var_exp, aes(x = `Number of dimension`, y = `Variance explained`)) +
  geom_col() + geom_line()
```

## Hiden signals

The primary objective of dimension reduction is to compress data to uncover patterns. Often after identifying patterns, it is of interest to explain their appearance, i.e. find the “hidden signal”. In data collection, we often collect additional information that are not included in the dimension reduction calculations but can hold the key to understanding these patterns. 
The simplest way to reveal such patterns is to use these external covariates is to include them in dimension reduction visualizations — with their values encoded as color, shape, size, or even transparency of corresponding points on the plot.

```{r}
pca_cereals_num$manufacturer <- cereals$Manufacturer
pca_cereals_num$type <- cereals$Cold.or.Hot

ggplot(pca_cereals_num, aes(
  x = PC1, y = PC2, label = label, col = manufacturer,
  shape = type
)) +
  geom_point(alpha = 0.75) +
  ggrepel::geom_text_repel(cex = 2.5)
```

We can see that the cereal are also clustering according to the manufacturer!

We could have saved some time by looking at who made the cereals, but only if we 
had some a-prior knowledge about cereals.




